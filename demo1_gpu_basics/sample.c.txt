// Global key word indicates this code runs on GPU
__global__
void add(int n, float a, float *x, float *y)
{
  int i = blockIdx.x*blockDim.x + threadIdx.x;
  if (i < n) y[i] = a*x[i] + y[i];
}

int main(void)
{

  // Initialise your array ( CPU memory)
  int N = 1<<20;
  float *x, *y, *d_x, *d_y;
  x = (float*)malloc(N*sizeof(float));
  y = (float*)malloc(N*sizeof(float));

  // Allocate the same sized array for GPU memory
  cudaMalloc(&d_x, N*sizeof(float));
  cudaMalloc(&d_y, N*sizeof(float));

  //Initialise array values
  for (int i = 0; i < N; i++) {
    x[i] = 1.0f;
    y[i] = 2.0f;
  }

  // Step 1 - Copy data from CPU (Host) memory to GPU (Device) memory
  cudaMemcpy(d_x, x, N*sizeof(float), cudaMemcpyHostToDevice);
  cudaMemcpy(d_y, y, N*sizeof(float), cudaMemcpyHostToDevice);

  // Step 2 Perform add on 1M elements on GPU
  add<<<(N+255)/256, 256>>>(N, 2.0f, d_x, d_y);

  // Step3 Copy data back from  GPU (Device) memory to CPU (Host) memory
  cudaMemcpy(y, d_y, N*sizeof(float), cudaMemcpyDeviceToHost);

  //
  float maxError = 0.0f;
  for (int i = 0; i < N; i++)
    maxError = max(maxError, abs(y[i]-4.0f));
  printf("Max error: %f\n", maxError);

  // Free memory on  GPU (Device)
  cudaFree(d_x);
  cudaFree(d_y);

    // Free memory on  Host (Device)
  free(x);
  free(y);
}